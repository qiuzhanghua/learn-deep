{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')   \n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import *\n",
    "\n",
    "class DRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(DRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) \n",
    "        # batch_first 为 True时output的tensor为（batch,seq,feature）,否则为（seq,batch,feature）\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        state = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # 计算输出和最终隐藏状态\n",
    "        output, _ = self.rnn(x, state)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "model = DRNN(16, 16, 64, 2)\n",
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True) # bidirectional为True是双向\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)  # 双向网络，因此有双倍hidden_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        state = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size) # 需要双倍的隐藏层\n",
    "        output, _ = self.rnn(x, state)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "model = BRNN(16, 16, 64, 2)\n",
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # LSTM\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "model = LSTM(16, 16, 64, 2)\n",
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True) # GRU\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "model = GRU(16, 16, 64, 2)\n",
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "dji = pdr.DataReader('^DJI', 'stooq')\n",
    "dji.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(dji['Close'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num = len(dji)                           # 总数据量\n",
    "x = torch.tensor(dji['Close'].to_list())  # 股价列表\n",
    "\n",
    "x = (x - torch.mean(x)) / torch.std(x)  #对数据进行归一化\n",
    "\n",
    "seq_len = 16                               # 预测序列长度\n",
    "batch_size = 16                            # 设置批大小\n",
    "\n",
    "X_feature = torch.zeros((num - seq_len, seq_len))      # 构建特征矩阵，num-seq_len行，seq_len列，初始值均为0\n",
    "Y_label = torch.zeros((num - seq_len, seq_len))        # 构建标签矩阵，形状同特征矩阵\n",
    "for i in range(seq_len):\n",
    "    X_feature[:, i] = x[i: num - seq_len + i]    # 为特征矩阵赋值\n",
    "    Y_label[:, i] = x[i+1: num - seq_len + i + 1]    # 为标签矩阵赋值\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(\n",
    "    X_feature[:num-seq_len].unsqueeze(2), Y_label[:num-seq_len]),\n",
    "    batch_size=batch_size, shuffle=True)  # 构建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_hiddens = 64\n",
    "n_layers = 2\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "# 建立模型\n",
    "model = DRNN(input_size, output_size, num_hiddens, n_layers)\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "trainer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "num_epochs = 20\n",
    "rnn_loss_history = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # 批量训练\n",
    "    for X, Y in train_loader:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred.squeeze(), Y.squeeze())\n",
    "        loss.sum().backward()\n",
    "        trainer.step()\n",
    "    # 输出损失\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for X, Y in train_loader:\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred.squeeze(), Y.squeeze())\n",
    "            total_loss += loss.sum()/loss.numel()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}: Validation loss = {avg_loss:.4f}')\n",
    "        rnn_loss_history.append(avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(loss_history, label='loss')\n",
    "plt.plot(rnn_loss_history, label='RNN_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_preds = model(X_feature.unsqueeze(2))\n",
    "rnn_preds.squeeze()\n",
    "time = torch.arange(1, num+1, dtype= torch.float32)  # 时间轴\n",
    "\n",
    "plt.plot(time[:num-seq_len], x[seq_len:num], label='dji')\n",
    "# plt.plot(time[:num-seq_len], preds.detach().numpy(), label='preds')\n",
    "plt.plot(time[:num-seq_len], rnn_preds[:,seq_len-1].detach(), label='RNN_preds')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_hiddens = 64\n",
    "n_layers = 2\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "# 建立模型\n",
    "model_name = ['DRNN', 'BRNN', 'LSTM', 'GRU']\n",
    "drnn = DRNN(input_size, output_size, num_hiddens, n_layers)\n",
    "brnn = BRNN(input_size, output_size, num_hiddens, n_layers)\n",
    "lstm = LSTM(input_size, output_size, num_hiddens, n_layers)\n",
    "gru = GRU(input_size, output_size, num_hiddens, n_layers)\n",
    "models = [drnn, brnn, lstm, gru]\n",
    "\n",
    "opts = [torch.optim.Adam(drnn.parameters(), lr), \n",
    "            torch.optim.Adam(brnn.parameters(), lr), \n",
    "            torch.optim.Adam(lstm.parameters(), lr), \n",
    "            torch.optim.Adam(gru.parameters(), lr)]\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "num_epochs = 20\n",
    "rnn_loss_history = []\n",
    "lr = 0.1\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # 批量训练\n",
    "    for X, Y in train_loader:\n",
    "        for index, model, optimizer in zip(range(len(models)), models, opts):\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred.squeeze(), Y.squeeze())\n",
    "            trainer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            trainer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    rnn_preds = models[i](X_feature.unsqueeze(2))\n",
    "    bias = torch.sum(x[seq_len:num] - rnn_preds[:,seq_len-1].detach().numpy())\n",
    "    print ('{} bias : {}'.format(model_name[i],str(bias)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
