{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engish to Chinese\n",
    "\n",
    "data from https://tatoeba.org/zh-cn/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')   \n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/eng2chs.tsv', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/eng2chs.tsv', encoding='utf-8') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line.strip().split('\\t')[1] + '\\t' + line.strip().split('\\t')[3])\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "content = ''.join(data)\n",
    "special_char = re.sub(r'[\\u4e00-\\u9fa5]', '', content)\n",
    "print(set(special_char) - set(string.ascii_letters) - set(string.digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    for i in range(len(data)):\n",
    "        # 替换特殊字符\n",
    "        data[i] = data[i].replace('\\u200b', '')\n",
    "        data[i] = data[i].replace('\\u200f', '')\n",
    "        data[i] = data[i].replace('\\xad', '')\n",
    "        data[i] = data[i].replace('\\u3000', ' ')\n",
    "        eng_mark = [',', '.', '!', '?'] # 因为标点前加空格\n",
    "        for mark in eng_mark:\n",
    "            data[i] = data[i].replace(mark, ' '+mark)\n",
    "        data[i] = data[i].lower()  # 统一替换为小写\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    # 分别存储源语言和目标语言的词元\n",
    "    src_tokens, tgt_tokens = [], []\n",
    "    for line in data:\n",
    "        pair = line.split('\\t')\n",
    "        src = pair[0].split(' ')\n",
    "        tgt = list(pair[1])\n",
    "        src_tokens.append(src)\n",
    "        tgt_tokens.append(tgt)\n",
    "    return src_tokens, tgt_tokens\n",
    "\n",
    "src_tokens, tgt_tokens = tokenize(data)\n",
    "print(\"src:\", src_tokens[:6])\n",
    "print(\"tgt:\", tgt_tokens[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def statistics(tokens):\n",
    "    max_len = 80 #只统计长度80以下的\n",
    "    len_list = range(max_len)  # 长度值\n",
    "    freq_list = np.zeros(max_len)  # 频率值\n",
    "    for token in tokens:\n",
    "        if len(token) < max_len:\n",
    "            freq_list[len_list.index(len(token))] += 1\n",
    "    return len_list, freq_list\n",
    "\n",
    "s1, s2 = statistics(src_tokens)\n",
    "t1, t2 = statistics(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(s1,s2)\n",
    "plt.plot(t1,t2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  #计数类\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]  #展平数组\n",
    "\n",
    "# 构建词表\n",
    "class Vocab:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens  # 传入的tokens是二维列表\n",
    "        self.token2index = {'<bos>': 0, '<eos>': 1, '<unk>':2, '<pad>':3}  # 先存好特殊词元\n",
    "        # 将词元按词频排序后生成列表\n",
    "        self.token2index.update({\n",
    "            token: index + 4\n",
    "            for index, (token, freq) in enumerate(\n",
    "                sorted(Counter(flatten(self.tokens)).items(), key=lambda x: x[1], reverse=True))\n",
    "        }) \n",
    "        #构建id到词元字典\n",
    "        self.index2token = {index: token for token, index in self.token2index.items()}\n",
    " \n",
    "    def __getitem__(self, query):\n",
    "        # 单一索引\n",
    "        if isinstance(query, (str, int)):\n",
    "            if isinstance(query, str):\n",
    "                return self.token2index.get(query, 0)\n",
    "            elif isinstance(query, (int)):\n",
    "                return self.index2token.get(query, '<unk>')\n",
    "        # 数组索引\n",
    "        elif isinstance(query, (list, tuple)):\n",
    "            return [self.__getitem__(item) for item in query]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.index2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "seq_len = 48  # 序列最大长度\n",
    "\n",
    "# 对数据按照最大长度进行截断和填充\n",
    "def padding(tokens, seq_len):\n",
    "    # 该函数针对单个句子进行处理\n",
    "    # 传入的句子是词元形式\n",
    "    return tokens[:seq_len] if len(tokens) > seq_len else tokens + ['<pad>'] * (seq_len - len(tokens))\n",
    "\n",
    "#实例化source和target词表\n",
    "src_vocab, tgt_vocab = Vocab(src_tokens), Vocab(tgt_tokens)\n",
    "\n",
    "#增加结尾标识<eos>\n",
    "src_data = torch.tensor([src_vocab[padding(line + ['<eos>'], seq_len)] for line in src_tokens])\n",
    "tgt_data = torch.tensor([tgt_vocab[padding(line + ['<eos>'], seq_len)] for line in tgt_tokens])\n",
    "\n",
    "# 训练集和测试集比例8比2，batch_size = 16\n",
    "train_size = int(len(src_data) * 0.8)\n",
    "test_size = len(src_data) - train_size\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(src_data[:train_size], tgt_data[:train_size]), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(src_data[-test_size:], tgt_data[-test_size:]), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器\n",
    "class Encoder(nn.Module):\n",
    " \n",
    "    def __init__(self, vocab_size, ebd_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, ebd_size, padding_idx=3)  # 将token表示为embedding\n",
    "        self.gru = nn.GRU(ebd_size, hidden_size, num_layers=num_layers)\n",
    " \n",
    "    def forward(self, encoder_inputs):\n",
    "        # encoder_inputs从(batch_size, seq_len)变成(batch_size, seq_len, emb_size)再调整为(seq_len, batch_size, emb_size)\n",
    "        encoder_inputs = self.embedding(encoder_inputs).permute(1, 0, 2)\n",
    "        output, hidden = self.gru(encoder_inputs)\n",
    "        # hidden 的形状为 (num_layers, batch_size, hidden_size)\n",
    "        # 最后时刻的最后一个隐层的输出的隐状态即为上下文向量\n",
    "        return hidden\n",
    "\n",
    "# 定义解码器\n",
    "class Decoder(nn.Module):\n",
    " \n",
    "    def __init__(self, vocab_size, ebd_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, ebd_size, padding_idx=3)\n",
    "        # 拼接维度ebd_size + hidden_size\n",
    "        self.gru = nn.GRU(ebd_size + hidden_size, hidden_size, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    " \n",
    "    def forward(self, decoder_inputs, encoder_states):\n",
    "        '''\n",
    "            decoder_inputs 为目标序列偏移一位的结果, 由初始形状: (batch_size, seq_len)变为(batch_size, seq_len)\n",
    "            再调整为(batch_size, seq_len, emb_size) -> (seq_len, batch_size, emb_size)\n",
    "        '''\n",
    "        decoder_inputs = self.embedding(decoder_inputs).permute(1, 0, 2)\n",
    "        context = encoder_states[-1] # 上下文向量取编码器的最后一个隐层的输出\n",
    "        # context 初始形状为 (batch_size, hidden_size)，为下一步连接，需repeat为(seq_len, batch_size, hidden_size)形式 \n",
    "        context = context.repeat(decoder_inputs.shape[0], 1, 1)\n",
    "        output, hidden = self.gru(torch.cat((decoder_inputs, context), -1), encoder_states)\n",
    "        # logits 的形状为 (seq_len, batch_size, vocab_size)\n",
    "        logits = self.linear(output)\n",
    "        return logits, hidden\n",
    "\n",
    "# seq2seq模型\n",
    "class Seq2Seq(nn.Module):\n",
    " \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    " \n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        return self.decoder(decoder_inputs, self.encoder(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "hidden_size = 256\n",
    "\n",
    "# 建立模型\n",
    "encoder = Encoder(len(src_vocab), len(src_vocab), hidden_size, num_layers=2)\n",
    "decoder = Decoder(len(tgt_vocab), len(tgt_vocab), hidden_size, num_layers=2)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# 交叉熵损失及adam优化器\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', ignore_index =3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# 记录损失变化\n",
    "loss_history = []\n",
    "\n",
    "#开始训练\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for encoder_inputs, decoder_targets in train_loader:\n",
    "        encoder_inputs, decoder_targets = encoder_inputs.to(device), decoder_targets.to(device)\n",
    "        # 偏移一位作为decoder的输入\n",
    "        # decoder的输入第一位是<bos>\n",
    "        bos_column = torch.tensor([tgt_vocab['<bos>']] * decoder_targets.shape[0]).reshape(-1, 1).to(device)\n",
    "        decoder_inputs = torch.cat((bos_column, decoder_targets[:, :-1]), dim=1)\n",
    "        # pred的形状为 (seq_len, batch_size, vocab_size)\n",
    "        pred, _ = model(encoder_inputs, decoder_inputs)\n",
    "        # decoder_targets 的形状为 (batch_size, seq_len)，我们需要改变pred的形状以保证它能够正确输入\n",
    "        # loss 的形状为 (batch_size, seq_len)，其中的每个元素都代表了一个词元的损失\n",
    "        loss = criterion(pred.permute(1, 2, 0), decoder_targets).mean()\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylabel('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/seq2seq_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# 计算bleu分数\n",
    "def bleu(label, pred, n):\n",
    "    score = math.exp(min(0, 1 - len(label) / len(pred)))\n",
    "    for k in range(1, n + 1):\n",
    "        num_matches = 0\n",
    "        hashtable = Counter([' '.join(label[i:i + k]) for i in range(len(label) - k + 1)])\n",
    "        for i in range(len(pred) - k + 1):\n",
    "            ngram = ' '.join(pred[i:i + k])\n",
    "            if ngram in hashtable and hashtable[ngram] > 0:\n",
    "                num_matches += 1\n",
    "                hashtable[ngram] -= 1\n",
    "        score *= pow(num_matches / (len(pred) - k + 1), pow(0.5, k))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "translation_results = []\n",
    "bleu_scores = []\n",
    "# 因为batch_size是1，所以每次取出来的都是单个句子\n",
    "for src_seq, tgt_seq in test_loader:\n",
    "    encoder_inputs = src_seq\n",
    "    hidden = model.encoder(encoder_inputs.to(device))\n",
    "    pred_seq = [tgt_vocab['<bos>']]\n",
    "    for _ in range(8):\n",
    "        # 一步步输出，decoder的输入的形状为(batch_size, seq_len)=(1,1)\n",
    "        decoder_inputs = torch.tensor(pred_seq[-1]).reshape(1, 1).to(device)\n",
    "        # pred形状为 (seq_len, batch_size, vocab_size) = (1, 1, vocab_size)\n",
    "        pred, hidden = model.decoder(decoder_inputs, hidden)\n",
    "        next_token_index = pred.squeeze().argmax().item()\n",
    "        if next_token_index == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        pred_seq.append(next_token_index)\n",
    "    \n",
    "    # 去掉开头的<bos>\n",
    "    pred_seq = tgt_vocab[pred_seq[1:]]\n",
    "    # 因为tgt_seq的形状为(1, seq_len)，我们需要将其转化成(seq_len, )的形状\n",
    "    tgt_seq = tgt_seq.squeeze().tolist()\n",
    "    \n",
    "    # 需要注意在<eos>之前截断\n",
    "    if tgt_vocab['<eos>'] in tgt_seq:\n",
    "        eos_idx = tgt_seq.index(tgt_vocab['<eos>'])\n",
    "        tgt_seq = tgt_vocab[tgt_seq[:eos_idx]]\n",
    "    else:\n",
    "        tgt_seq = tgt_vocab[tgt_seq]\n",
    "    translation_results.append((' '.join(tgt_seq), ' '.join(pred_seq)))\n",
    "    bleu_scores.append(bleu(tgt_seq, pred_seq, n=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "print(sum(bleu_scores) / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
