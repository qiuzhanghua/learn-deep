{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降过程\n",
    "#### 1. 二维平面内的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')   \n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 4 * x + 1\n",
    "\n",
    "x = torch.tensor(-10.0, requires_grad=True, device=device)\n",
    "learning_rate = 0.8\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for i in range(100):\n",
    "    y = f(x)\n",
    "    xs.append(x.item())\n",
    "    ys.append(y.item())\n",
    "    y.backward()\n",
    "    with torch.no_grad():\n",
    "      x.data -= learning_rate * x.grad\n",
    "      x.grad.zero_()\n",
    "\n",
    "print(f'最终参数：{x.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin = torch.arange(-10, 10, 0.1)\n",
    "y_origin = f(x_origin)\n",
    "plt.plot(x_origin.numpy(), y_origin.numpy(), 'b-')\n",
    "\n",
    "plt.plot(xs, ys, 'r--')\n",
    "plt.scatter(xs, ys, s = 50, c='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 三维平面内的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + 2 * y**2\n",
    "\n",
    "x = torch.tensor(-10.0, requires_grad=True, device=device)\n",
    "y = torch.tensor(-10.0, requires_grad=True, device=device)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "zs = []\n",
    "\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    z = f(x, y)\n",
    "    xs.append(x.item())\n",
    "    ys.append(y.item())\n",
    "    zs.append(z.item())\n",
    "    z.backward()\n",
    "    with torch.no_grad():\n",
    "        x.data -= learning_rate * x.grad\n",
    "        y.data -= learning_rate * y.grad\n",
    "        x.grad.zero_()\n",
    "        y.grad.zero_()\n",
    "        \n",
    "print(f'最终参数：x={x.item()}, y={y.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.plot(xs, ys, zs, 'r-')\n",
    "ax.scatter(xs, ys, zs, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = torch.meshgrid(torch.arange(-10, 10, 0.1), torch.arange(-10, 10, 0.1), indexing='ij')\n",
    "Z = f(X, Y)\n",
    "\n",
    "plt.contour(X.numpy(), Y.numpy(), Z.numpy(), levels=30)\n",
    "\n",
    "plt.plot(xs, ys, 'r-')\n",
    "plt.scatter(xs, ys, c='r', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化器对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + 2 * y**2\n",
    "\n",
    "n_samples = 1000\n",
    "X = torch.rand(n_samples)\n",
    "Y = torch.rand(n_samples)\n",
    "Z = f(X, Y) + torch.randn(n_samples)\n",
    "\n",
    "dataset = torch.stack([X, Y, Z], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(n_samples * 0.8)\n",
    "test_size = n_samples - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, lengths=[train_size, test_size])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "  TensorDataset(train_dataset.dataset.narrow(1,0,2), train_dataset.dataset.narrow(1,2,1)), \n",
    "  batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(\n",
    "  TensorDataset(test_dataset.dataset.narrow(1,0,2), test_dataset.dataset.narrow(1,2,1)), \n",
    "  batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 8)\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt_labels = ['SGD', 'Momemtum', 'Adagrad', 'RMSprop', 'Adadelta', 'Adam']\n",
    "models = [Model() for _ in range(len(opt_labels))]\n",
    "\n",
    "SGD = optim.SGD(models[0].parameters(), lr=learning_rate)\n",
    "Momentum = optim.SGD(models[1].parameters(), lr=learning_rate, momentum=0.8, nesterov=True)\n",
    "Adagrad = optim.Adagrad(models[2].parameters(), lr=learning_rate)\n",
    "RMSprop = optim.RMSprop(models[3].parameters(), lr=learning_rate)\n",
    "Adadelta = optim.Adadelta(models[4].parameters(), lr=learning_rate)\n",
    "Adam = optim.Adam(models[5].parameters(), lr=learning_rate)\n",
    "\n",
    "optimizers = [SGD, Momentum, Adagrad, RMSprop, Adadelta, Adam]\n",
    "\n",
    "train_losses_history = [[] for _ in range(len(opt_labels))]\n",
    "test_losses_history = [[] for _ in range(len(opt_labels))]\n",
    "n_epochs = 50\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_losses = [0 for _ in range(len(opt_labels))]\n",
    "    for i, (X_batch, Y_batch) in enumerate(train_dataloader):\n",
    "        for j, model in enumerate(models):\n",
    "            model.train()\n",
    "            optimizer = optimizers[j]\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_fn(Y_pred, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses[j] += loss.item()\n",
    "\n",
    "    test_losses = [0 for _ in range(len(opt_labels))]     \n",
    "    for i, (X_batch, Y_batch) in enumerate(test_dataloader):\n",
    "        for j, model in enumerate(models):\n",
    "            model.eval()\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_fn(Y_pred, Y_batch)\n",
    "            test_losses[j] += loss.item()\n",
    "\n",
    "    for i in range(len(opt_labels)):\n",
    "        train_losses[i] /= len(train_dataloader)\n",
    "        train_losses_history[i].append(train_losses[i])\n",
    "        test_losses[i] /= len(test_dataloader)\n",
    "        test_losses_history[i].append(test_losses[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l_his in enumerate(train_losses_history):\n",
    "    plt.plot(l_his, label=opt_labels[i])\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l_his in enumerate(test_losses_history):\n",
    "    plt.plot(l_his, label=opt_labels[i])\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
